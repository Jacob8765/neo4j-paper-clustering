{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neo4j.api.ServerInfo at 0x28bf37dd0>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = GraphDatabase.driver(os.getenv(\"NEO4J_URI\"), auth=(os.getenv(\"NEO4J_USER\"), os.getenv(\"NEO4J_PASS\")))\n",
    "driver.get_server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the papers to the database\n",
    "import json\n",
    "PAPERS_DATA_PATH = \"extract/papers_data.json\"\n",
    "\n",
    "with open(PAPERS_DATA_PATH, 'r') as json_file:\n",
    "        papers_data = json.load(json_file)\n",
    "        #remove the papers with no abstract\n",
    "        papers_data = [paper for paper in papers_data if len(paper['abstract']) > 0]\n",
    "\n",
    "CIPHR = \"\"\"\n",
    "WITH $data as data\n",
    "UNWIND data as paper\n",
    "MERGE (p:Paper {title: paper.title})\n",
    "SET p.abstract = paper.abstract\n",
    "SET p.url = paper.url\n",
    "SET p.embeddings = paper.abstract_embedding\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(CIPHR, data=papers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector index on the embeddings property\n",
    "#in neo4j you can do this with \n",
    "# CALL db.index.vector.createNodeIndex('paper-embeddings', 'Paper', 'embeddings', 1536, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 19\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Assuming you have already created a Neo4j driver instance\n",
    "# driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "GET_PAPERS = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.embeddings IS NOT NULL\n",
    "RETURN p.title as title, p.abstract as abstract, p.url as url, p.embeddings as embeddings\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    papers_result = session.run(GET_PAPERS)\n",
    "    papers = [dict(record) for record in papers_result]\n",
    "    print(f\"Number of papers: {len(papers)}\")\n",
    "\n",
    "GET_CENTROIDS = \"\"\"\n",
    "MATCH (c:Centroid)\n",
    "RETURN c.coordinates as coordinates, c.title as centroid_title\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    centroids_result = session.run(GET_CENTROIDS)\n",
    "    centroids = [dict(record) for record in centroids_result]\n",
    "\n",
    "# papers, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 18 nearest neighbors...\n",
      "[t-SNE] Indexed 19 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 19 samples in 0.002s...\n",
      "[t-SNE] Computed conditional probabilities for sample 19 / 19\n",
      "[t-SNE] Mean sigma: 0.236983\n",
      "[t-SNE] Computed conditional probabilities in 0.001s\n",
      "[t-SNE] Iteration 50: error = 51.2412262, gradient norm = 0.6367951 (50 iterations in 0.233s)\n",
      "[t-SNE] Iteration 100: error = 44.0365677, gradient norm = 0.8259702 (50 iterations in 0.016s)\n",
      "[t-SNE] Iteration 150: error = 52.8435020, gradient norm = 0.7618183 (50 iterations in 0.017s)\n",
      "[t-SNE] Iteration 200: error = 55.4319839, gradient norm = 0.4608711 (50 iterations in 0.011s)\n",
      "[t-SNE] Iteration 250: error = 53.9757156, gradient norm = 0.5957789 (50 iterations in 0.011s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 53.975716\n",
      "[t-SNE] Iteration 300: error = 0.7895068, gradient norm = 0.0080028 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 350: error = 0.5224281, gradient norm = 0.0177655 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 400: error = 0.3300363, gradient norm = 0.0061736 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 450: error = 0.2940420, gradient norm = 0.0038372 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 500: error = 0.2739302, gradient norm = 0.0067278 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 550: error = 0.2652049, gradient norm = 0.0060280 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 600: error = 0.2351681, gradient norm = 0.0011890 (50 iterations in 0.011s)\n",
      "[t-SNE] Iteration 650: error = 0.2309667, gradient norm = 0.0043828 (50 iterations in 0.008s)\n",
      "[t-SNE] Iteration 700: error = 0.2126449, gradient norm = 0.0017099 (50 iterations in 0.011s)\n",
      "[t-SNE] Iteration 750: error = 0.2096697, gradient norm = 0.0032880 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 800: error = 0.1909131, gradient norm = 0.0019660 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 850: error = 0.1884550, gradient norm = 0.0016827 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 900: error = 0.1860362, gradient norm = 0.0020440 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 950: error = 0.1813664, gradient norm = 0.0020620 (50 iterations in 0.012s)\n",
      "[t-SNE] Iteration 1000: error = 0.1811787, gradient norm = 0.0005552 (50 iterations in 0.010s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.181179\n"
     ]
    }
   ],
   "source": [
    "#run TSNE on the node and centroid embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#convert the embeddings to a numpy array\n",
    "embeddings = np.array([paper['embeddings'] for paper in papers])\n",
    "\n",
    "#run TSNE on the node embeddings\n",
    "tsne = TSNE(n_components=2, perplexity=10, verbose=2, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "#save the TSNE results back to the database\n",
    "UPDATE_TSNE = \"\"\"\n",
    "UNWIND $data as row\n",
    "MATCH (p:Paper {title: row.title})\n",
    "SET p.tsne_x = row.x, p.tsne_y = row.y\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(UPDATE_TSNE, data=[{'title': paper['title'], 'x': tsne_result[0], 'y': tsne_result[1]} for paper, tsne_result in zip(papers, tsne_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 2, 2, 0, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 0, 1, 1],\n",
       "       dtype=int32),\n",
       " array([[ 0.00539881,  0.00177483,  0.01381221, ..., -0.02469022,\n",
       "         -0.01093476, -0.02433426],\n",
       "        [ 0.00114015,  0.00892018,  0.00723498, ..., -0.00510286,\n",
       "         -0.01842937, -0.01463698],\n",
       "        [-0.01615786,  0.01661973, -0.00431059, ..., -0.01056143,\n",
       "         -0.00387151, -0.01288979]]))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "K=3\n",
    "\n",
    "#convert the centroids to a numpy array\n",
    "centroid_embeddings = np.array([centroid['coordinates'] for centroid in centroids])\n",
    "\n",
    "# if there aren't enough centroids, initialize the rest randomly\n",
    "if len(centroids) < K:\n",
    "    # Initialize the missing centroids randomly\n",
    "    if centroid_embeddings.shape[0] == 0:\n",
    "        centroid_embeddings = \"k-means++\"\n",
    "    else:\n",
    "        centroid_embeddings = np.vstack([centroid_embeddings, np.random.rand(K - len(centroids), 1536)])\n",
    "\n",
    "#initialize the k-means algorithm\n",
    "kmeans = KMeans(n_clusters=K, init=centroid_embeddings, n_init=1)\n",
    "\n",
    "#fit the algorithm to the data\n",
    "predictions = kmeans.fit_predict(embeddings)\n",
    "predictions, kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the centroids to the database\n",
    "ADD_CENTROIDS = \"\"\"\n",
    "UNWIND $centroids as centroid\n",
    "MERGE (c:Centroid {id: centroid.id})\n",
    "SET c.coordinates = centroid.coordinates\n",
    "\"\"\"\n",
    "\n",
    "centroids = [{'id': i, 'coordinates': centroid } for i, centroid in enumerate(kmeans.cluster_centers_)]\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(ADD_CENTROIDS, centroids=centroids)\n",
    "\n",
    "#add the cluster relationships to the database\n",
    "ADD_CLUSTER_RELATIONSHIPS = \"\"\"\n",
    "UNWIND $data as d\n",
    "MATCH (p:Paper {title: d.title})\n",
    "MATCH (c:Centroid {id: d.cluster})\n",
    "MERGE (p)-[:PART_OF_CLUSTER]->(c)\n",
    "\"\"\"\n",
    "\n",
    "data = [{'title': paper['title'], 'cluster': prediction} for paper, prediction in zip(papers, predictions)]\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(ADD_CLUSTER_RELATIONSHIPS, data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster_id': 0,\n",
       "  'nearest_nodes': [{'title': 'Learning from nature to enhance Blue engineering of marine infrastructure',\n",
       "    'score': 0.9769815802574158},\n",
       "   {'title': 'Getting into the groove: Opportunities to enhance the ecological value of hard coastal infrastructure using Ô¨Åne-scale surface textures',\n",
       "    'score': 0.9756118059158325},\n",
       "   {'title': 'Availability of microhabitats explains a widespread pattern and informs theory on ecological engineering of boulder reefs',\n",
       "    'score': 0.9705075025558472}]},\n",
       " {'cluster_id': 1,\n",
       "  'nearest_nodes': [{'title': 'Biocement Fabrication and Design Application for a Sustainable Urban Area',\n",
       "    'score': 0.9727442264556885},\n",
       "   {'title': 'Characteristics of bio-CaCO 3 from microbial bio-mineralization with different bacteria species',\n",
       "    'score': 0.9710575938224792},\n",
       "   {'title': 'The study of long-term durability and bio-colonization of concrete in marine environment',\n",
       "    'score': 0.9639110565185547}]},\n",
       " {'cluster_id': 2,\n",
       "  'nearest_nodes': [{'title': 'Engineered Living Materials: Prospects and Challenges for Using Biological Systems to Direct the Assembly of Smart Materials',\n",
       "    'score': 0.9792594909667969},\n",
       "   {'title': 'Engineered Living Materials: Taxonomies and Emerging Trends',\n",
       "    'score': 0.9676738381385803},\n",
       "   {'title': 'Intelligent and smart biomaterials for sustainable 3D printing applications',\n",
       "    'score': 0.9676399827003479}]}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute the title of each centroid\n",
    "NEAREST_NODES = \"\"\"\n",
    "match(c:Centroid)\n",
    "call db.index.vector.queryNodes('paper-embeddings', 3, c.coordinates) YIELD node, score\n",
    "RETURN c.id as cluster_id, COLLECT({title: node.title, score: score}) AS nearest_nodes\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(NEAREST_NODES, centroids=centroids)\n",
    "    centroids_with_nearest_nodes = [dict(record) for record in result]\n",
    "\n",
    "centroids_with_nearest_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': [{'cluster_id': 0,\n",
       "   'title': 'Ecological Engineering of Marine Infrastructure'},\n",
       "  {'cluster_id': 1, 'title': 'Sustainable Bio-Cement and Concrete'},\n",
       "  {'cluster_id': 2,\n",
       "   'title': 'Engineered Living Materials and Smart Biomaterials'}]}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "#langchain configuration\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"labels\", description=\"An array of objects of schema {cluster_id: int, title: str} representing the labels of each cluster. Only give one title per cluster. You are given data of a handful of papers, as well as the cluster they belong to. The score is a number betweeo 0-1 giving the confidence of the model that the paper belongs to the cluster. Make your labels general, remember you are only given a subset of the papers, there are many more and the label should represent all of them\"),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "    \"We are clustering research papers based on their embeddings. You are given the titles of some research papers, as well as their distance to the cluster. Give each cluster a unique name that is representative of the data it is close to. .\\n{format_instructions}\\n{clusters}\")]\n",
    "    ,\n",
    "    input_variables=[\"clusters\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "_input = prompt.format_prompt(clusters=centroids_with_nearest_nodes)\n",
    "output = chat_model(_input.to_messages())\n",
    "labels = output_parser.parse(output.content)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the labels, update the database\n",
    "UPDATE_CENTROID_TITLES = \"\"\"\n",
    "UNWIND $labels as label\n",
    "MATCH (c:Centroid {id: label.cluster_id})\n",
    "SET c.title = label.title\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(UPDATE_CENTROID_TITLES, labels=labels.get('labels'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
