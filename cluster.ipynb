{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neo4j.api.ServerInfo at 0x28beeac90>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = GraphDatabase.driver(os.getenv(\"NEO4J_URI\"), auth=(os.getenv(\"NEO4J_USER\"), os.getenv(\"NEO4J_PASS\")))\n",
    "driver.get_server_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the papers to the database\n",
    "import json\n",
    "PAPERS_DATA_PATH = \"extract/papers_data.json\"\n",
    "\n",
    "with open(PAPERS_DATA_PATH, 'r') as json_file:\n",
    "        papers_data = json.load(json_file)\n",
    "        #remove the papers with no abstract\n",
    "        papers_data = [paper for paper in papers_data if len(paper['abstract']) > 0]\n",
    "\n",
    "CIPHR = \"\"\"\n",
    "WITH $data as data\n",
    "UNWIND data as paper\n",
    "MERGE (p:Paper {title: paper.title})\n",
    "SET p.abstract = paper.abstract\n",
    "SET p.url = paper.url\n",
    "SET p.embeddings = paper.abstract_embedding\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(CIPHR, data=papers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector index on the embeddings property\n",
    "#in neo4j you can do this with \n",
    "# CALL db.index.vector.createNodeIndex('paper-embeddings', 'Paper', 'embeddings', 1536, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 19\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Assuming you have already created a Neo4j driver instance\n",
    "# driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "GET_PAPERS = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.embeddings IS NOT NULL\n",
    "RETURN p.title as title, p.abstract as abstract, p.url as url, p.embeddings as embeddings\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    papers_result = session.run(GET_PAPERS)\n",
    "    papers = [dict(record) for record in papers_result]\n",
    "    print(f\"Number of papers: {len(papers)}\")\n",
    "\n",
    "GET_CENTROIDS = \"\"\"\n",
    "MATCH (c:Centroid)\n",
    "RETURN c.coordinates as coordinates, c.title as centroid_title\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    centroids_result = session.run(GET_CENTROIDS)\n",
    "    centroids = [dict(record) for record in centroids_result]\n",
    "\n",
    "# papers, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 18 nearest neighbors...\n",
      "[t-SNE] Indexed 19 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 19 samples in 0.086s...\n",
      "[t-SNE] Computed conditional probabilities for sample 19 / 19\n",
      "[t-SNE] Mean sigma: 0.236983\n",
      "[t-SNE] Computed conditional probabilities in 0.001s\n",
      "[t-SNE] Iteration 50: error = 53.9376297, gradient norm = 0.7883933 (50 iterations in 0.273s)\n",
      "[t-SNE] Iteration 100: error = 50.3236237, gradient norm = 0.7446774 (50 iterations in 0.011s)\n",
      "[t-SNE] Iteration 150: error = 49.1673355, gradient norm = 0.4904715 (50 iterations in 0.011s)\n",
      "[t-SNE] Iteration 200: error = 53.2442169, gradient norm = 1.0219053 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 250: error = 53.1747208, gradient norm = 0.3859081 (50 iterations in 0.010s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 53.174721\n",
      "[t-SNE] Iteration 300: error = 0.7295227, gradient norm = 0.0105972 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 350: error = 0.3333885, gradient norm = 0.0120932 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 400: error = 0.2023649, gradient norm = 0.0054934 (50 iterations in 0.011s)\n",
      "[t-SNE] Iteration 450: error = 0.1762851, gradient norm = 0.0006835 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 500: error = 0.1752450, gradient norm = 0.0012217 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 550: error = 0.1730582, gradient norm = 0.0037672 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 600: error = 0.1620811, gradient norm = 0.0040143 (50 iterations in 0.008s)\n",
      "[t-SNE] Iteration 650: error = 0.1581303, gradient norm = 0.0011147 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 700: error = 0.1584641, gradient norm = 0.0003667 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 750: error = 0.1584657, gradient norm = 0.0004569 (50 iterations in 0.010s)\n",
      "[t-SNE] Iteration 800: error = 0.1584602, gradient norm = 0.0006844 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 850: error = 0.1584524, gradient norm = 0.0009430 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 900: error = 0.1584421, gradient norm = 0.0012239 (50 iterations in 0.011s)\n",
      "[t-SNE] Iteration 950: error = 0.1584263, gradient norm = 0.0015619 (50 iterations in 0.009s)\n",
      "[t-SNE] Iteration 1000: error = 0.1584913, gradient norm = 0.0018414 (50 iterations in 0.012s)\n",
      "[t-SNE] Iteration 1000: did not make any progress during the last 300 episodes. Finished.\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.158491\n"
     ]
    }
   ],
   "source": [
    "#run TSNE on the node and centroid embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#convert the embeddings to a numpy array\n",
    "embeddings = np.array([paper['embeddings'] for paper in papers])\n",
    "\n",
    "#run TSNE on the node embeddings\n",
    "tsne = TSNE(n_components=2, perplexity=10, verbose=2, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "#save the TSNE results back to the database\n",
    "UPDATE_TSNE = \"\"\"\n",
    "UNWIND $data as row\n",
    "MATCH (p:Paper {title: row.title})\n",
    "SET p.tsne_x = row.x, p.tsne_y = row.y\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(UPDATE_TSNE, data=[{'title': paper['title'], 'x': tsne_result[0], 'y': tsne_result[1]} for paper, tsne_result in zip(papers, tsne_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 2, 2, 2, 1, 2, 1, 1, 1, 0, 2, 1, 2, 1, 0, 1, 2, 2, 2],\n",
       "       dtype=int32),\n",
       " array([[ 0.00957119,  0.00659721,  0.01460397, ...,  0.00216391,\n",
       "         -0.02703943, -0.01040091],\n",
       "        [ 0.00425553,  0.00378272,  0.01124918, ..., -0.0180465 ,\n",
       "         -0.01018701, -0.0188333 ],\n",
       "        [-0.00924649,  0.0132581 ,  0.00046706, ..., -0.01001863,\n",
       "         -0.01145077, -0.01652182]]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "K=3\n",
    "\n",
    "#convert the centroids to a numpy array\n",
    "centroid_embeddings = np.array([centroid['coordinates'] for centroid in centroids])\n",
    "\n",
    "# if there aren't enough centroids, initialize the rest randomly\n",
    "if len(centroids) < K:\n",
    "    # Initialize the missing centroids randomly\n",
    "    if centroid_embeddings.shape[0] == 0:\n",
    "        centroid_embeddings = \"kmeans++\"\n",
    "    else:\n",
    "        centroid_embeddings = np.vstack([centroid_embeddings, np.random.rand(K - len(centroids), 1536)])\n",
    "\n",
    "#initialize the k-means algorithm\n",
    "kmeans = KMeans(n_clusters=K, init=centroid_embeddings, n_init=1)\n",
    "\n",
    "#fit the algorithm to the data\n",
    "predictions = kmeans.fit_predict(embeddings)\n",
    "predictions, kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the centroids to the database\n",
    "ADD_CENTROIDS = \"\"\"\n",
    "UNWIND $centroids as centroid\n",
    "MERGE (c:Centroid {id: centroid.id})\n",
    "SET c.coordinates = centroid.coordinates\n",
    "\"\"\"\n",
    "\n",
    "centroids = [{'id': i, 'coordinates': centroid } for i, centroid in enumerate(kmeans.cluster_centers_)]\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(ADD_CENTROIDS, centroids=centroids)\n",
    "\n",
    "#add the cluster relationships to the database\n",
    "ADD_CLUSTER_RELATIONSHIPS = \"\"\"\n",
    "UNWIND $data as d\n",
    "MATCH (p:Paper {title: d.title})\n",
    "MATCH (c:Centroid {id: d.cluster})\n",
    "MERGE (p)-[:PART_OF_CLUSTER]->(c)\n",
    "\"\"\"\n",
    "\n",
    "data = [{'title': paper['title'], 'cluster': prediction} for paper, prediction in zip(papers, predictions)]\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(ADD_CLUSTER_RELATIONSHIPS, data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster_id': 0,\n",
       "  'nearest_nodes': [{'title': 'Microbially Induced Carbonate Precipitation Using Microorganisms Enriched from Calcareous Materials in Marine Environments and Their Metabolites',\n",
       "    'score': 0.989421010017395},\n",
       "   {'title': 'Characteristics of bio-CaCO 3 from microbial bio-mineralization with different bacteria species',\n",
       "    'score': 0.9894202947616577},\n",
       "   {'title': 'Biocement Fabrication and Design Application for a Sustainable Urban Area',\n",
       "    'score': 0.9602454900741577}]},\n",
       " {'cluster_id': 1,\n",
       "  'nearest_nodes': [{'title': 'Getting into the groove: Opportunities to enhance the ecological value of hard coastal infrastructure using ﬁne-scale surface textures',\n",
       "    'score': 0.9751214981079102},\n",
       "   {'title': 'Learning from nature to enhance Blue engineering of marine infrastructure',\n",
       "    'score': 0.9691004753112793},\n",
       "   {'title': 'Availability of microhabitats explains a widespread pattern and informs theory on ecological engineering of boulder reefs',\n",
       "    'score': 0.9644978046417236}]},\n",
       " {'cluster_id': 2,\n",
       "  'nearest_nodes': [{'title': 'Engineered Living Materials: Prospects and Challenges for Using Biological Systems to Direct the Assembly of Smart Materials',\n",
       "    'score': 0.9669255018234253},\n",
       "   {'title': 'Biocement Fabrication and Design Application for a Sustainable Urban Area',\n",
       "    'score': 0.9639508128166199},\n",
       "   {'title': 'Intelligent and smart biomaterials for sustainable 3D printing applications',\n",
       "    'score': 0.9600445628166199}]}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute the title of each centroid\n",
    "NEAREST_NODES = \"\"\"\n",
    "match(c:Centroid)\n",
    "call db.index.vector.queryNodes('paper-embeddings', 3, c.coordinates) YIELD node, score\n",
    "RETURN c.id as cluster_id, COLLECT({title: node.title, score: score}) AS nearest_nodes\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(NEAREST_NODES, centroids=centroids)\n",
    "    centroids_with_nearest_nodes = [dict(record) for record in result]\n",
    "\n",
    "centroids_with_nearest_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': [{'cluster_id': 0, 'title': 'Microbial Carbonate Precipitation'},\n",
       "  {'cluster_id': 1,\n",
       "   'title': 'Enhancing Ecological Value of Coastal Infrastructure'},\n",
       "  {'cluster_id': 2, 'title': 'Engineered Living Materials'}]}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "#langchain configuration\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"labels\", description=\"An array of objects of schema {cluster_id: int, title: str} representing the labels of each cluster. Only give one title per cluster. You are given data of a handful of papers, as well as the cluster they belong to. The score is a number betweeo 0-1 giving the confidence of the model that the paper belongs to the cluster. Make your labels general, remember you are only given a subset of the papers, there are many more and the label should represent all of them\"),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "    \"We are clustering research papers based on their embeddings. You are given the titles of some research papers, as well as their distance to the cluster. Give each cluster a unique name that is representative of the data it is close to. .\\n{format_instructions}\\n{clusters}\")]\n",
    "    ,\n",
    "    input_variables=[\"clusters\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "_input = prompt.format_prompt(clusters=centroids_with_nearest_nodes)\n",
    "output = chat_model(_input.to_messages())\n",
    "labels = output_parser.parse(output.content)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the labels, update the database\n",
    "UPDATE_CENTROID_TITLES = \"\"\"\n",
    "UNWIND $labels as label\n",
    "MATCH (c:Centroid {id: label.cluster_id})\n",
    "SET c.title = label.title\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(UPDATE_CENTROID_TITLES, labels=labels.get('labels'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
